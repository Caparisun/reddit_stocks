{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-nicaragua",
   "metadata": {},
   "source": [
    "# Scraping data from Reddits Wallstreetbets subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greater-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from pandas import DataFrame \n",
    "import praw\n",
    "from praw.models import MoreComments \n",
    "import re\n",
    "import requests     \n",
    "from textblob import TextBlob \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attractive-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common words that are also stock tickers or definetly uneccesary. Excluding them from the analysis since the context is hard to fig\n",
    "exclude = ['VERY', 'A', 'B', 'GO', 'ARE', 'ON', 'FOR', 'THE', 'TO', ' ', 'SO', 'IT','AT', 'BE', 'OR', 'SO', 'ALL', 'HAS', 'BY', 'CAN', 'AN', 'OUT', 'NOW']\n",
    "now = datetime.now() # set time to now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "permanent-welding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n"
     ]
    }
   ],
   "source": [
    "# setting up redit client\n",
    "reddit = praw.Reddit(\n",
    "  client_id = \"dlY27DaxJQaL5Q\",\n",
    "  client_secret = \"z-MStmsM-inT4-XJmeGtovN05XCEgw\",\n",
    "  user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to create a frequency table of the scraped comments\n",
    "def frequency_table_comments(clean):\n",
    "    for (index, row) in clean.iterrows(): # iterate over dataframe\n",
    "  # titles \n",
    "        title = row['comments'].upper() # get title in lowercase\n",
    "        title = regex.sub('', title)  # clean with reges\n",
    "        title_words = title.split(' ') # split titles at whitespace\n",
    "    for words in title_words:\n",
    "        if x in exclude: # common words that are also stock tickers or definetly uneccesary. \n",
    "            word_dict[x] += 1\n",
    "        else:\n",
    "            word_dict[x] = 1\n",
    "    return pd.DataFrame.from_dict(list(word_dict.items())).rename(columns = {0:\"Term\", 1:\"Frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "iraqi-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining funtion to append comments to a list and creating a dataframe from them\n",
    "def appending_to_list(submission):\n",
    "    comments= []\n",
    "    for top_level_comment in submission.comments[:-1]: #leaving out the last comment, since it creates an error\n",
    "        comments.append(top_level_comment.body) # append comment to list\n",
    "# return dataframe of the list\n",
    "    return DataFrame(comments,columns=['comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-affiliation",
   "metadata": {},
   "source": [
    "# Step 1: Scrape posts into dataframe. \n",
    "Reddit divides posts within a subreddit (think subforum) into multiple categories, such as new, hot and top posts.\n",
    "The cell below scrapes these exact categories in the 'wallstreetbets' subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elder-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting posts into a dataframe\n",
    "df = [] # define empty list that will hold dictionarys\n",
    "#scraper =     \n",
    "for post in reddit.subreddit('wallstreetbets').hot(limit=500): # call wallstreetbets subreddit \"hot\" section and get first 1000 posts\n",
    "    content = {  # create dictionary for results\n",
    "    \"title\" : post.title, # store title\n",
    "    \"text\" : post.selftext # store text of the post\n",
    "    }\n",
    "    df.append(content) # append dataframe\n",
    "for post in reddit.subreddit('wallstreetbets').new(limit=2000): # call wallstreetbets subreddit \"new\" section and get first 1000 posts\n",
    "    content = {  # create dictionary for results\n",
    "    \"title\" : post.title, # store title\n",
    "    \"text\" : post.selftext # store text of the post\n",
    "    }\n",
    "    df.append(content) # append dataframe\n",
    "for post in reddit.subreddit('wallstreetbets').top(limit=500): # call wallstreetbets subreddit \"top\" section and get first 1000 posts\n",
    "    content = {  # create dictionary for results\n",
    "    \"title\" : post.title, # store title\n",
    "    \"text\" : post.selftext # store text of the post\n",
    "    }\n",
    "    df.append(content) # append dataframe\n",
    "    df_posts = pd.DataFrame(df) # convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-gardening",
   "metadata": {},
   "source": [
    "# Step 2: Create frequency table of unique words and the number of their mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effective-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z ]') # remove everything thats not a letter or space aka numbers and emojis\n",
    "word_dict = {} # create dictionary\n",
    "for (index, row) in df_posts.iterrows(): # iterate over dataframe\n",
    "  # titles \n",
    "    title = row['title'].upper() # get title in lowercase\n",
    "    title = regex.sub('', title)  # clean with reges\n",
    "    title_words = title.split(' ') # split titles at whitespace\n",
    "  # content\n",
    "    content = row['text'].upper() # get text from post in lowercase\n",
    "    content = regex.sub('', content) # clean with regex\n",
    "    content_words = content.split(' ') # split titles at whitespace\n",
    "  # combine titles and comments\n",
    "    words = title_words + content_words\n",
    "    for x in words:\n",
    "        if x in exclude:\n",
    "            pass\n",
    "        elif x in word_dict:\n",
    "            word_dict[x] += 1\n",
    "        else:\n",
    "            word_dict[x] = 1\n",
    "posts_freq= pd.DataFrame.from_dict(list(word_dict.items())).rename(columns = {0:\"Term\", 1:\"Frequency\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-campus",
   "metadata": {},
   "source": [
    "## We now have a frequency table of the most often used words in the top 500 hot posts\n",
    "### Next up is scraping the top level comments from the daily discussion thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stuffed-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments of daily discussion thread\n",
    "url = (\"https://www.reddit.com/r/wallstreetbets/comments/n9th6n/daily_discussion_thread_for_\"+\n",
    "str(now.strftime(\"%B\")).lower() + \"_\"+  # constructing the link with daytime function\n",
    "str(now.strftime(\"%d\")).lower() +\"_\" +  # since this post is created new on a daily basis\n",
    "str(now.strftime(\"%Y\")).lower() + \"/\")  # Month, day, year\n",
    "\n",
    "submission = reddit.submission(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adapted-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append these comments to a list\n",
    "daily_comments= [] # create empty list\n",
    "for top_level_comment in submission.comments[:-1]: #leaving out the last comment, since it creates an error\n",
    "        daily_comments.append(top_level_comment.body) # append comment to list\n",
    "len(daily_comments)\n",
    "\n",
    "# create dataframe of the comments\n",
    "df_comments = DataFrame(daily_comments,columns=['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preceding-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = appending_to_list(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "precious-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling cleaning/splitting function to create frequency table\n",
    "comments_freq = frequency_table_comments(df_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-burden",
   "metadata": {},
   "source": [
    "## Scraping the \"what are your moves\" thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "third-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments of daily discussion thread\n",
    "url = (\"https://www.reddit.com/r/wallstreetbets/comments/n9eiyu/what_are_your_moves_tomorrow_\"+\n",
    "str(now.strftime(\"%B\")).lower() + \"_\"+  # constructing the link with daytime function\n",
    "str(now.strftime(\"%d\")).lower() +\"_\" +  # since this post is created new on a daily basis\n",
    "str(now.strftime(\"%Y\")).lower() + \"/\")  # Month, day, year\n",
    "submission = reddit.submission(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "balanced-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_move = appending_to_list(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "atlantic-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling cleaning/splitting function from above\n",
    "moves_freq = frequency_table_comments(df_comments_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "waiting-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all frequency tables on the term, performing an inner join \n",
    "merged_table = pd.merge(pd.merge(posts_freq,comments_freq,on='Term', how='left'),moves_freq,on='Term', how = 'left')\n",
    "# replace NA#s with zeros\n",
    "merged_table = merged_table.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controlled-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating overall frequency in a new column\n",
    "merged_table['frequency']=merged_table['Frequency_x'] + merged_table['Frequency_y'] + merged_table['Frequency']\n",
    "# drop unnecessary frequency columns, only keeping the main one\n",
    "merged_table = merged_table.drop(['Frequency_x', 'Frequency_y', 'Frequency'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "handmade-possession",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies Inc. Common Stock</td>\n",
       "      <td>Capital Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation Common Stock</td>\n",
       "      <td>Basic Industries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAC</td>\n",
       "      <td>Ares Acquisition Corporation Class A Ordinary ...</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>ATA Creativity Global American Depositary Shares</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACQ</td>\n",
       "      <td>Artius Acquisition Inc. Class A Common Stock</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7636</th>\n",
       "      <td>ZWRKW</td>\n",
       "      <td>Z-Work Acquisition Corp. Warrant</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>ZY</td>\n",
       "      <td>Zymergen Inc. Common Stock</td>\n",
       "      <td>Basic Industries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7638</th>\n",
       "      <td>ZYME</td>\n",
       "      <td>Zymeworks Inc. Common Shares</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7639</th>\n",
       "      <td>ZYNE</td>\n",
       "      <td>Zynerba Pharmaceuticals Inc. Common Stock</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7640</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex Inc. Common Stock</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7641 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term                                       Company_Name  \\\n",
       "0         A             Agilent Technologies Inc. Common Stock   \n",
       "1        AA                    Alcoa Corporation Common Stock    \n",
       "2       AAC  Ares Acquisition Corporation Class A Ordinary ...   \n",
       "3      AACG   ATA Creativity Global American Depositary Shares   \n",
       "4      AACQ       Artius Acquisition Inc. Class A Common Stock   \n",
       "...     ...                                                ...   \n",
       "7636  ZWRKW                   Z-Work Acquisition Corp. Warrant   \n",
       "7637     ZY                         Zymergen Inc. Common Stock   \n",
       "7638   ZYME                       Zymeworks Inc. Common Shares   \n",
       "7639   ZYNE          Zynerba Pharmaceuticals Inc. Common Stock   \n",
       "7640   ZYXI                            Zynex Inc. Common Stock   \n",
       "\n",
       "                Sector  \n",
       "0        Capital Goods  \n",
       "1     Basic Industries  \n",
       "2              Finance  \n",
       "3        Miscellaneous  \n",
       "4              Finance  \n",
       "...                ...  \n",
       "7636           Finance  \n",
       "7637  Basic Industries  \n",
       "7638               NaN  \n",
       "7639       Health Care  \n",
       "7640       Health Care  \n",
       "\n",
       "[7641 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing a tickerlist from the nasdaq\n",
    "ticker_df = pd.read_csv('tickers2.csv', index_col=None, delimiter=';').rename(columns = {\"Symbol\":\"Term\", \"Name\":\"Company_Name\"})\n",
    "ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "previous-garage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>frequency</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEST</td>\n",
       "      <td>1017</td>\n",
       "      <td>BEST Inc. American Depositary Shares each repr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DD</td>\n",
       "      <td>600</td>\n",
       "      <td>DuPont de Nemours Inc. Common Stock</td>\n",
       "      <td>Capital Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TH</td>\n",
       "      <td>348</td>\n",
       "      <td>Target Hospitality Corp. Common Stock</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLAY</td>\n",
       "      <td>552</td>\n",
       "      <td>Dave &amp; Buster's Entertainment Inc. Common Stock</td>\n",
       "      <td>Consumer Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIS</td>\n",
       "      <td>33</td>\n",
       "      <td>Walt Disney Company (The) Common Stock</td>\n",
       "      <td>Consumer Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>PLOW</td>\n",
       "      <td>3</td>\n",
       "      <td>Douglas Dynamics Inc. Common Stock</td>\n",
       "      <td>Capital Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>PACK</td>\n",
       "      <td>3</td>\n",
       "      <td>Ranpak Holdings Corp Class A Common Stock</td>\n",
       "      <td>Basic Industries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>PMD</td>\n",
       "      <td>3</td>\n",
       "      <td>Psychemedics Corporation</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>GF</td>\n",
       "      <td>3</td>\n",
       "      <td>New Germany Fund Inc. (The) Common Stock</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>BBBY</td>\n",
       "      <td>3</td>\n",
       "      <td>Bed Bath &amp; Beyond Inc. Common Stock</td>\n",
       "      <td>Consumer Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Term  frequency                                       Company_Name  \\\n",
       "0    BEST       1017  BEST Inc. American Depositary Shares each repr...   \n",
       "1      DD        600                DuPont de Nemours Inc. Common Stock   \n",
       "2      TH        348              Target Hospitality Corp. Common Stock   \n",
       "3    PLAY        552    Dave & Buster's Entertainment Inc. Common Stock   \n",
       "4     DIS         33             Walt Disney Company (The) Common Stock   \n",
       "..    ...        ...                                                ...   \n",
       "627  PLOW          3                 Douglas Dynamics Inc. Common Stock   \n",
       "628  PACK          3          Ranpak Holdings Corp Class A Common Stock   \n",
       "629   PMD          3                           Psychemedics Corporation   \n",
       "630    GF          3           New Germany Fund Inc. (The) Common Stock   \n",
       "631  BBBY          3                Bed Bath & Beyond Inc. Common Stock   \n",
       "\n",
       "                Sector  \n",
       "0                  NaN  \n",
       "1        Capital Goods  \n",
       "2               Energy  \n",
       "3    Consumer Services  \n",
       "4    Consumer Services  \n",
       "..                 ...  \n",
       "627      Capital Goods  \n",
       "628   Basic Industries  \n",
       "629        Health Care  \n",
       "630                NaN  \n",
       "631  Consumer Services  \n",
       "\n",
       "[632 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging tickerlist with mentions on reddit, inner join, so we will loose all words that are not tickers and all tickers that arent mentioned\n",
    "stonks_df = pd.merge(merged_table, ticker_df, on=\"Term\")\n",
    "stonks_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "other-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting data to csv\n",
    "stonks_df.to_csv('frequency_of_tickers_' + str(now.strftime(\"%d\"))+'_'+str(now.strftime(\"%m\"))+ '_'+ str(now.strftime(\"%H\"))+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-migration",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-affect",
   "metadata": {},
   "source": [
    "### Adding sentiment to full comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "theoretical-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tickers that are in the above dataframe\n",
    "tickers = ticker_df['Term'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hungarian-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Vader to analyse social media sentiment better than with nltk\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerous-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    try:\n",
    "        scores = sia.polarity_scores(text)\n",
    "        return  scores['compound']\n",
    "    except: return none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "religious-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polarity column to posts\n",
    "df_posts['polarity'] = df_posts['text'].apply(sentiment)\n",
    "# drop title column\n",
    "df_posts = df_posts.drop(['title'], axis=1)\n",
    "# add polarity column \n",
    "df_comments['polarity'] = df_comments['comments'].apply(sentiment)\n",
    "df_comments_move['polarity'] = df_comments_move['comments'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "associate-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of the three dataframes to make iterating for sentiment extraction easier\n",
    "list_df = [df_posts, df_comments, df_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "controlling-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting sentiment and associated ticker out of the dataframe\n",
    "sentiment_to_df= [] # list for sentiment score\n",
    "tickers_to_df = [] # list for tickers\n",
    "\n",
    "for dataframe in list_df: \n",
    "    for index, row in dataframe.iterrows(): # for every row in dataframe\n",
    "        title_words = row[0].split(' ') # split comment at whitespace and put into list\n",
    "        for word in title_words:\n",
    "            if word in tickers: \n",
    "                tickers_to_df.append(word) # append the word to the word list\n",
    "                sentiment_to_df.append(row[1]) # append the sentiment to the sentiment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alien-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentiment dataframe \n",
    "sentiment_df1 = pd.DataFrame(list(zip(tickers_to_df, sentiment_to_df)),\n",
    "              columns =['Term', 'Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "mounted-dairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576174</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.919900</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.123243</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994700</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998200</td>\n",
       "      <td>AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.378600</td>\n",
       "      <td>XL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.975067</td>\n",
       "      <td>XOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.894600</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.943700</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.957300</td>\n",
       "      <td>ZM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Polarity  Term\n",
       "0    0.576174     A\n",
       "1    0.919900   AAL\n",
       "2   -0.123243  AAPL\n",
       "3    0.994700  ABNB\n",
       "4    0.998200    AC\n",
       "..        ...   ...\n",
       "245  0.378600    XL\n",
       "246  0.975067   XOM\n",
       "247  0.894600     Y\n",
       "248  0.943700     Z\n",
       "249  0.957300    ZM\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group table by ticker, get mean of sentiment, create new dataframe with new index\n",
    "sentiment_table=sentiment_df1.groupby(['Term']).mean()\n",
    "sentiment_table['Term'] = sentiment_table.index\n",
    "sentiment_table=sentiment_table.reset_index(drop=True)\n",
    "sentiment_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "colored-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "sentiment_table.to_csv('sentiment_of_tickers_' + str(now.strftime(\"%d\"))+'_'+str(now.strftime(\"%m\"))+ '_'+ str(now.strftime(\"%H\"))+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sensitive-monroe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e5c41087c332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fourier 25'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-spare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1620939902212,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
