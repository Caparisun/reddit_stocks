{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-nicaragua",
   "metadata": {},
   "source": [
    "# Scraping data from Reddits Wallstreetbets subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "greater-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from pandas import DataFrame \n",
    "import praw\n",
    "from praw.models import MoreComments \n",
    "import re\n",
    "import requests     \n",
    "from textblob import TextBlob \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "attractive-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common words that are also stock tickers or definetly uneccesary. Excluding them from the analysis since the context is hard to fig\n",
    "exclude = ['VERY', 'A', 'B', 'GO', 'ARE', 'ON', 'FOR', 'THE', 'TO', ' ', 'SO', 'IT','AT', 'BE', 'OR', 'SO', 'ALL', 'HAS', 'BY', 'CAN', 'AN', 'OUT', 'NOW']\n",
    "now = datetime.now() # set time to now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "permanent-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up redit client\n",
    "reddit = praw.Reddit(\n",
    "  client_id = \"dlY27DaxJQaL5Q\",\n",
    "  client_secret = \"z-MStmsM-inT4-XJmeGtovN05XCEgw\",\n",
    "  user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "comparable-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to create a frequency table of the scraped comments\n",
    "def frequency_table_comments(clean):\n",
    "    for (index, row) in clean.iterrows(): # iterate over dataframe\n",
    "  # titles \n",
    "        title = row['comments'].upper() # get title in lowercase\n",
    "        title = regex.sub('', title)  # clean with reges\n",
    "        title_words = title.split(' ') # split titles at whitespace\n",
    "    for words in title_words:\n",
    "        if x in exclude: # common words that are also stock tickers or definetly uneccesary. Excluding them from the analysis since the context is hard to figure out\n",
    "            pass\n",
    "        elif x in word_dict:\n",
    "            word_dict[x] += 1\n",
    "        else:\n",
    "            word_dict[x] = 1\n",
    "    return pd.DataFrame.from_dict(list(word_dict.items())).rename(columns = {0:\"Term\", 1:\"Frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "iraqi-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining funtion to append comments to a list and creating a dataframe from them\n",
    "def appending_to_list(submission):\n",
    "    comments= []\n",
    "    for top_level_comment in submission.comments[:-1]: #leaving out the last comment, since it creates an error\n",
    "        comments.append(top_level_comment.body) # append comment to list\n",
    "# return dataframe of the list\n",
    "    return DataFrame(comments,columns=['comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-affiliation",
   "metadata": {},
   "source": [
    "# Step 1: Scrape posts into dataframe. \n",
    "Reddit divides posts within a subreddit (think subforum) into multiple categories, such as new, hot and top posts.\n",
    "The cell below scrapes these exact categories in the 'wallstreetbets' subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "elder-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting posts into a dataframe\n",
    "df = [] # define empty list that will hold dictionarys\n",
    "#scraper =     \n",
    "for post in reddit.subreddit('wallstreetbets').hot(limit=500): # call wallstreetbets subreddit \"hot\" section and get first 1000 posts\n",
    "    content = {  # create dictionary for results\n",
    "    \"title\" : post.title, # store title\n",
    "    \"text\" : post.selftext # store text of the post\n",
    "    }\n",
    "    df.append(content) # append dataframe\n",
    "for post in reddit.subreddit('wallstreetbets').new(limit=2000): # call wallstreetbets subreddit \"new\" section and get first 1000 posts\n",
    "    content = {  # create dictionary for results\n",
    "    \"title\" : post.title, # store title\n",
    "    \"text\" : post.selftext # store text of the post\n",
    "    }\n",
    "    df.append(content) # append dataframe\n",
    "for post in reddit.subreddit('wallstreetbets').top(limit=500): # call wallstreetbets subreddit \"top\" section and get first 1000 posts\n",
    "    content = {  # create dictionary for results\n",
    "    \"title\" : post.title, # store title\n",
    "    \"text\" : post.selftext # store text of the post\n",
    "    }\n",
    "    df.append(content) # append dataframe\n",
    "    df_posts = pd.DataFrame(df) # convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-gardening",
   "metadata": {},
   "source": [
    "# Step 2: Create frequency table of unique words and the number of their mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "effective-correlation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHAT</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOUR</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOVES</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOMORROW</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAY</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14465</th>\n",
       "      <td>LEGION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14466</th>\n",
       "      <td>ORGANISATION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14467</th>\n",
       "      <td>STANDALONES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14468</th>\n",
       "      <td>ITGODSPEED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14469</th>\n",
       "      <td>PROUDLY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14470 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Term  Frequency\n",
       "0              WHAT        560\n",
       "1              YOUR        486\n",
       "2             MOVES         62\n",
       "3          TOMORROW         72\n",
       "4               MAY        365\n",
       "...             ...        ...\n",
       "14465        LEGION          1\n",
       "14466  ORGANISATION          1\n",
       "14467   STANDALONES          1\n",
       "14468    ITGODSPEED          1\n",
       "14469       PROUDLY          1\n",
       "\n",
       "[14470 rows x 2 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('[^a-zA-Z ]') # remove everything thats not a letter or space aka numbers and emojis\n",
    "word_dict = {} # create dictionary\n",
    "for (index, row) in df_posts.iterrows(): # iterate over dataframe\n",
    "  # titles \n",
    "    title = row['title'].upper() # get title in lowercase\n",
    "    title = regex.sub('', title)  # clean with reges\n",
    "    title_words = title.split(' ') # split titles at whitespace\n",
    "  # content\n",
    "    content = row['text'].upper() # get text from post in lowercase\n",
    "    content = regex.sub('', content) # clean with regex\n",
    "    content_words = content.split(' ') # split titles at whitespace\n",
    "  # combine titles and comments\n",
    "    words = title_words + content_words\n",
    "    for x in words:\n",
    "        if x in exclude:\n",
    "            pass\n",
    "        elif x in word_dict:\n",
    "            word_dict[x] += 1\n",
    "        else:\n",
    "            word_dict[x] = 1\n",
    "posts_freq= pd.DataFrame.from_dict(list(word_dict.items())).rename(columns = {0:\"Term\", 1:\"Frequency\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-campus",
   "metadata": {},
   "source": [
    "## We now have a frequency table of the most often used words in the top 500 hot posts\n",
    "### Next up is scraping the top level comments from the daily discussion thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "stuffed-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments of daily discussion thread\n",
    "url = (\"https://www.reddit.com/r/wallstreetbets/comments/n9th6n/daily_discussion_thread_for_\"+\n",
    "str(now.strftime(\"%B\")).lower() + \"_\"+  # constructing the link with daytime function\n",
    "str(now.strftime(\"%d\")).lower() +\"_\" +  # since this post is created new on a daily basis\n",
    "str(now.strftime(\"%Y\")).lower() + \"/\")  # Month, day, year\n",
    "\n",
    "submission = reddit.submission(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "adapted-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append these comments to a list\n",
    "daily_comments= [] # create empty list\n",
    "for top_level_comment in submission.comments[:-1]: #leaving out the last comment, since it creates an error\n",
    "        daily_comments.append(top_level_comment.body) # append comment to list\n",
    "len(daily_comments)\n",
    "\n",
    "# create dataframe of the comments\n",
    "df_comments = DataFrame(daily_comments,columns=['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "preceding-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = appending_to_list(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "precious-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling cleaning/splitting function to create frequency table\n",
    "comments_freq = frequency_table_comments(df_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-burden",
   "metadata": {},
   "source": [
    "## Scraping the \"what are your moves\" thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "third-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments of daily discussion thread\n",
    "url = (\"https://www.reddit.com/r/wallstreetbets/comments/n9eiyu/what_are_your_moves_tomorrow_\"+\n",
    "str(now.strftime(\"%B\")).lower() + \"_\"+  # constructing the link with daytime function\n",
    "str(now.strftime(\"%d\")).lower() +\"_\" +  # since this post is created new on a daily basis\n",
    "str(now.strftime(\"%Y\")).lower() + \"/\")  # Month, day, year\n",
    "submission = reddit.submission(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "balanced-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_move = appending_to_list(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "atlantic-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling cleaning/splitting function from above\n",
    "moves_freq = frequency_table_comments(df_comments_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "waiting-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all frequency tables on the term, performing an inner join \n",
    "merged_table = pd.merge(pd.merge(posts_freq,comments_freq,on='Term', how='left'),moves_freq,on='Term', how = 'left')\n",
    "# replace NA#s with zeros\n",
    "merged_table = merged_table.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "controlled-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating overall frequency in a new column\n",
    "merged_table['frequency']=merged_table['Frequency_x'] + merged_table['Frequency_y'] + merged_table['Frequency']\n",
    "# drop unnecessary frequency columns, only keeping the main one\n",
    "merged_table = merged_table.drop(['Frequency_x', 'Frequency_y', 'Frequency'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "handmade-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a tickerlist from the nasdaq\n",
    "ticker_df = pd.read_csv('tickers.csv').rename(columns = {\"Symbol\":\"Term\", \"Name\":\"Company_Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "previous-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging tickerlist with mentions on reddit, inner join, so we will loose all words that are not tickers and all tickers that arent mentioned\n",
    "stonks_df = pd.merge(merged_table, ticker_df, on=\"Term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "other-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting data to csv\n",
    "stonks_df.to_csv('frequency_of_tickers_' + str(now.strftime(\"%d\"))+'_'+str(now.strftime(\"%m\"))+ '_'+ str(now.strftime(\"%H\"))+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-migration",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-affect",
   "metadata": {},
   "source": [
    "### Adding sentiment to full comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "theoretical-aside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'AA',\n",
       " 'AAC',\n",
       " 'AACG',\n",
       " 'AACQ',\n",
       " 'AACQU',\n",
       " 'AACQW',\n",
       " 'AAIC',\n",
       " 'AAIC^B',\n",
       " 'AAIC^C',\n",
       " 'AAL',\n",
       " 'AAMC',\n",
       " 'AAME',\n",
       " 'AAN',\n",
       " 'AAOI',\n",
       " 'AAON',\n",
       " 'AAP',\n",
       " 'AAPL',\n",
       " 'AAT',\n",
       " 'AAU',\n",
       " 'AAWW',\n",
       " 'AB',\n",
       " 'ABB',\n",
       " 'ABBV',\n",
       " 'ABC',\n",
       " 'ABCB',\n",
       " 'ABCL',\n",
       " 'ABCM',\n",
       " 'ABEO',\n",
       " 'ABEV',\n",
       " 'ABG',\n",
       " 'ABGI',\n",
       " 'ABIO',\n",
       " 'ABM',\n",
       " 'ABMD',\n",
       " 'ABNB',\n",
       " 'ABR',\n",
       " 'ABR^A',\n",
       " 'ABR^B',\n",
       " 'ABR^C',\n",
       " 'ABST',\n",
       " 'ABT',\n",
       " 'ABTX',\n",
       " 'ABUS',\n",
       " 'AC',\n",
       " 'ACA',\n",
       " 'ACAC',\n",
       " 'ACACU',\n",
       " 'ACACW',\n",
       " 'ACAD',\n",
       " 'ACAH',\n",
       " 'ACAHU',\n",
       " 'ACAHW',\n",
       " 'ACB',\n",
       " 'ACBAU',\n",
       " 'ACBI',\n",
       " 'ACC',\n",
       " 'ACCD',\n",
       " 'ACCO',\n",
       " 'ACEL',\n",
       " 'ACER',\n",
       " 'ACET',\n",
       " 'ACEV',\n",
       " 'ACEVU',\n",
       " 'ACEVW',\n",
       " 'ACGL',\n",
       " 'ACGLO',\n",
       " 'ACGLP',\n",
       " 'ACH',\n",
       " 'ACHC',\n",
       " 'ACHL',\n",
       " 'ACHV',\n",
       " 'ACI',\n",
       " 'ACIC',\n",
       " 'ACII',\n",
       " 'ACIU',\n",
       " 'ACIW',\n",
       " 'ACKIT',\n",
       " 'ACKIU',\n",
       " 'ACKIW',\n",
       " 'ACLS',\n",
       " 'ACM',\n",
       " 'ACMR',\n",
       " 'ACN',\n",
       " 'ACNB',\n",
       " 'ACND',\n",
       " 'ACOR',\n",
       " 'ACP',\n",
       " 'ACQRU',\n",
       " 'ACR',\n",
       " 'ACR^C',\n",
       " 'ACRE',\n",
       " 'ACRS',\n",
       " 'ACRX',\n",
       " 'ACST',\n",
       " 'ACTC',\n",
       " 'ACTCU',\n",
       " 'ACTCW',\n",
       " 'ACTDU',\n",
       " 'ACTG',\n",
       " 'ACU',\n",
       " 'ACV',\n",
       " 'ACVA',\n",
       " 'ACY',\n",
       " 'ADAG',\n",
       " 'ADAP',\n",
       " 'ADBE',\n",
       " 'ADC',\n",
       " 'ADCT',\n",
       " 'ADER',\n",
       " 'ADERU',\n",
       " 'ADERW',\n",
       " 'ADES',\n",
       " 'ADEX',\n",
       " 'ADFI',\n",
       " 'ADI',\n",
       " 'ADIL',\n",
       " 'ADILW',\n",
       " 'ADM',\n",
       " 'ADMA',\n",
       " 'ADME',\n",
       " 'ADMP',\n",
       " 'ADMS',\n",
       " 'ADN',\n",
       " 'ADNT',\n",
       " 'ADNWW',\n",
       " 'ADOC',\n",
       " 'ADOCR',\n",
       " 'ADOCW',\n",
       " 'ADP',\n",
       " 'ADPT',\n",
       " 'ADRA',\n",
       " 'ADS',\n",
       " 'ADSK',\n",
       " 'ADT',\n",
       " 'ADTN',\n",
       " 'ADTX',\n",
       " 'ADUS',\n",
       " 'ADV',\n",
       " 'ADVM',\n",
       " 'ADVWW',\n",
       " 'ADX',\n",
       " 'ADXN',\n",
       " 'ADXS',\n",
       " 'AE',\n",
       " 'AEAC',\n",
       " 'AEACU',\n",
       " 'AEACW',\n",
       " 'AEB',\n",
       " 'AEE',\n",
       " 'AEF',\n",
       " 'AEFC',\n",
       " 'AEG',\n",
       " 'AEGN',\n",
       " 'AEHL',\n",
       " 'AEHR',\n",
       " 'AEI',\n",
       " 'AEIS',\n",
       " 'AEL',\n",
       " 'AEL^A',\n",
       " 'AEL^B',\n",
       " 'AEM',\n",
       " 'AEMD',\n",
       " 'AENZ',\n",
       " 'AEO',\n",
       " 'AEP',\n",
       " 'AEPPL',\n",
       " 'AEPPZ',\n",
       " 'AER',\n",
       " 'AERI',\n",
       " 'AES',\n",
       " 'AESC',\n",
       " 'AESE',\n",
       " 'AESR',\n",
       " 'AEVA',\n",
       " 'AEY',\n",
       " 'AEYE',\n",
       " 'AEZS',\n",
       " 'AFAQU',\n",
       " 'AFB',\n",
       " 'AFBI',\n",
       " 'AFCG',\n",
       " 'AFG',\n",
       " 'AFGB',\n",
       " 'AFGC',\n",
       " 'AFGD',\n",
       " 'AFGE',\n",
       " 'AFI',\n",
       " 'AFIB',\n",
       " 'AFIN',\n",
       " 'AFINO',\n",
       " 'AFINP',\n",
       " 'AFL',\n",
       " 'AFMD',\n",
       " 'AFRM',\n",
       " 'AFT',\n",
       " 'AFYA',\n",
       " 'AG',\n",
       " 'AGAC',\n",
       " 'AGBA',\n",
       " 'AGBAR',\n",
       " 'AGBAW',\n",
       " 'AGC',\n",
       " 'AGCB',\n",
       " 'AGCO',\n",
       " 'AGCUU',\n",
       " 'AGCWW',\n",
       " 'AGD',\n",
       " 'AGE',\n",
       " 'AGEN',\n",
       " 'AGFS',\n",
       " 'AGFY',\n",
       " 'AGGRU',\n",
       " 'AGGRW',\n",
       " 'AGI',\n",
       " 'AGIO',\n",
       " 'AGL',\n",
       " 'AGLE',\n",
       " 'AGM',\n",
       " 'AGM^C',\n",
       " 'AGM^D',\n",
       " 'AGM^E',\n",
       " 'AGM^F',\n",
       " 'AGMH',\n",
       " 'AGNC',\n",
       " 'AGNCM',\n",
       " 'AGNCN',\n",
       " 'AGNCO',\n",
       " 'AGNCP',\n",
       " 'AGO',\n",
       " 'AGO^B',\n",
       " 'AGO^E',\n",
       " 'AGO^F',\n",
       " 'AGR',\n",
       " 'AGRO',\n",
       " 'AGRX',\n",
       " 'AGS',\n",
       " 'AGTC',\n",
       " 'AGTI',\n",
       " 'AGX',\n",
       " 'AGYS',\n",
       " 'AHAC',\n",
       " 'AHACU',\n",
       " 'AHACW',\n",
       " 'AHC',\n",
       " 'AHCO',\n",
       " 'AHH',\n",
       " 'AHH^A',\n",
       " 'AHL^C',\n",
       " 'AHL^D',\n",
       " 'AHL^E',\n",
       " 'AHPI',\n",
       " 'AHT',\n",
       " 'AHT^D',\n",
       " 'AHT^F',\n",
       " 'AHT^G',\n",
       " 'AHT^H',\n",
       " 'AHT^I',\n",
       " 'AI',\n",
       " 'AIC',\n",
       " 'AIF',\n",
       " 'AIG',\n",
       " 'AIG^A',\n",
       " 'AIH',\n",
       " 'AIHS',\n",
       " 'AIKI',\n",
       " 'AIM',\n",
       " 'AIMC',\n",
       " 'AIN',\n",
       " 'AINC',\n",
       " 'AINV',\n",
       " 'AIO',\n",
       " 'AIR',\n",
       " 'AIRC',\n",
       " 'AIRG',\n",
       " 'AIRI',\n",
       " 'AIRT',\n",
       " 'AIRTP',\n",
       " 'AIRTW',\n",
       " 'AIT',\n",
       " 'AIV',\n",
       " 'AIW',\n",
       " 'AIZ',\n",
       " 'AIZN',\n",
       " 'AJAX',\n",
       " 'AJG',\n",
       " 'AJRD',\n",
       " 'AJX',\n",
       " 'AJXA',\n",
       " 'AKAM',\n",
       " 'AKBA',\n",
       " 'AKIC',\n",
       " 'AKICU',\n",
       " 'AKICW',\n",
       " 'AKO/A',\n",
       " 'AKO/B',\n",
       " 'AKR',\n",
       " 'AKRO',\n",
       " 'AKTS',\n",
       " 'AKTX',\n",
       " 'AKU',\n",
       " 'AKUS',\n",
       " 'AKYA',\n",
       " 'AL',\n",
       " 'AL^A',\n",
       " 'ALAC',\n",
       " 'ALACR',\n",
       " 'ALACW',\n",
       " 'ALB',\n",
       " 'ALBO',\n",
       " 'ALC',\n",
       " 'ALCO',\n",
       " 'ALDX',\n",
       " 'ALE',\n",
       " 'ALEC',\n",
       " 'ALEX',\n",
       " 'ALF',\n",
       " 'ALFIW',\n",
       " 'ALG',\n",
       " 'ALGM',\n",
       " 'ALGN',\n",
       " 'ALGS',\n",
       " 'ALGT',\n",
       " 'ALHC',\n",
       " 'ALIM',\n",
       " 'ALIN^A',\n",
       " 'ALIN^B',\n",
       " 'ALIN^E',\n",
       " 'ALJJ',\n",
       " 'ALK',\n",
       " 'ALKS',\n",
       " 'ALKT',\n",
       " 'ALL',\n",
       " 'ALL^B',\n",
       " 'ALL^G',\n",
       " 'ALL^H',\n",
       " 'ALL^I',\n",
       " 'ALLE',\n",
       " 'ALLK',\n",
       " 'ALLO',\n",
       " 'ALLT',\n",
       " 'ALLY',\n",
       " 'ALLY^A',\n",
       " 'ALNA',\n",
       " 'ALNY',\n",
       " 'ALOT',\n",
       " 'ALP^Q',\n",
       " 'ALPN',\n",
       " 'ALRM',\n",
       " 'ALRN',\n",
       " 'ALRS',\n",
       " 'ALSK',\n",
       " 'ALSN',\n",
       " 'ALT',\n",
       " 'ALTA',\n",
       " 'ALTG',\n",
       " 'ALTG^A',\n",
       " 'ALTM',\n",
       " 'ALTO',\n",
       " 'ALTR',\n",
       " 'ALTU',\n",
       " 'ALTUU',\n",
       " 'ALTUW',\n",
       " 'ALUS',\n",
       " 'ALV',\n",
       " 'ALVR',\n",
       " 'ALX',\n",
       " 'ALXN',\n",
       " 'ALXO',\n",
       " 'ALYA',\n",
       " 'AM',\n",
       " 'AMAL',\n",
       " 'AMAO',\n",
       " 'AMAOU',\n",
       " 'AMAOW',\n",
       " 'AMAT',\n",
       " 'AMBA',\n",
       " 'AMBC',\n",
       " 'AMBO',\n",
       " 'AMC',\n",
       " 'AMCR',\n",
       " 'AMCX',\n",
       " 'AMD',\n",
       " 'AME',\n",
       " 'AMED',\n",
       " 'AMEH',\n",
       " 'AMER',\n",
       " 'AMG',\n",
       " 'AMGN',\n",
       " 'AMH',\n",
       " 'AMH^D',\n",
       " 'AMH^E',\n",
       " 'AMH^F',\n",
       " 'AMH^G',\n",
       " 'AMH^H',\n",
       " 'AMHC',\n",
       " 'AMHCU',\n",
       " 'AMHCW',\n",
       " 'AMK',\n",
       " 'AMKR',\n",
       " 'AMN',\n",
       " 'AMNB',\n",
       " 'AMOT',\n",
       " 'AMOV',\n",
       " 'AMP',\n",
       " 'AMPE',\n",
       " 'AMPG',\n",
       " 'AMPGW',\n",
       " 'AMPH',\n",
       " 'AMPI',\n",
       " 'AMPY',\n",
       " 'AMR',\n",
       " 'AMRB',\n",
       " 'AMRC',\n",
       " 'AMRK',\n",
       " 'AMRN',\n",
       " 'AMRS',\n",
       " 'AMRX',\n",
       " 'AMS',\n",
       " 'AMSC',\n",
       " 'AMSF',\n",
       " 'AMST',\n",
       " 'AMSWA',\n",
       " 'AMT',\n",
       " 'AMTB',\n",
       " 'AMTBB',\n",
       " 'AMTI',\n",
       " 'AMTX',\n",
       " 'AMWD',\n",
       " 'AMWL',\n",
       " 'AMX',\n",
       " 'AMYT',\n",
       " 'AMZN',\n",
       " 'AN',\n",
       " 'ANAB',\n",
       " 'ANAC',\n",
       " 'ANAT',\n",
       " 'ANDA',\n",
       " 'ANDAR',\n",
       " 'ANDAW',\n",
       " 'ANDE',\n",
       " 'ANEB',\n",
       " 'ANET',\n",
       " 'ANF',\n",
       " 'ANGI',\n",
       " 'ANGN',\n",
       " 'ANGO',\n",
       " 'ANIK',\n",
       " 'ANIP',\n",
       " 'ANIX',\n",
       " 'ANNX',\n",
       " 'ANPC',\n",
       " 'ANSS',\n",
       " 'ANTE',\n",
       " 'ANTM',\n",
       " 'ANVS',\n",
       " 'ANY',\n",
       " 'ANZU',\n",
       " 'ANZUU',\n",
       " 'ANZUW',\n",
       " 'AOD',\n",
       " 'AON',\n",
       " 'AONE',\n",
       " 'AOS',\n",
       " 'AOSL',\n",
       " 'AOUT',\n",
       " 'AP',\n",
       " 'APA',\n",
       " 'APAM',\n",
       " 'APD',\n",
       " 'APDN',\n",
       " 'APEI',\n",
       " 'APEN',\n",
       " 'APG',\n",
       " 'APGB',\n",
       " 'APH',\n",
       " 'API',\n",
       " 'APLE',\n",
       " 'APLS',\n",
       " 'APLT',\n",
       " 'APM',\n",
       " 'APO',\n",
       " 'APO^A',\n",
       " 'APO^B',\n",
       " 'APOG',\n",
       " 'APOP',\n",
       " 'APOPW',\n",
       " 'APP',\n",
       " 'APPF',\n",
       " 'APPH',\n",
       " 'APPHW',\n",
       " 'APPN',\n",
       " 'APPS',\n",
       " 'APR',\n",
       " 'APRE',\n",
       " 'APRN',\n",
       " 'APRZ',\n",
       " 'APSG',\n",
       " 'APT',\n",
       " 'APTO',\n",
       " 'APTS',\n",
       " 'APTV',\n",
       " 'APTV^A',\n",
       " 'APTX',\n",
       " 'APVO',\n",
       " 'APWC',\n",
       " 'APXT',\n",
       " 'APXTU',\n",
       " 'APXTW',\n",
       " 'APYX',\n",
       " 'AQB',\n",
       " 'AQMS',\n",
       " 'AQN',\n",
       " 'AQNA',\n",
       " 'AQNB',\n",
       " 'AQST',\n",
       " 'AQUA',\n",
       " 'AR',\n",
       " 'ARAV',\n",
       " 'ARAY',\n",
       " 'ARBG',\n",
       " 'ARBGU',\n",
       " 'ARBGW',\n",
       " 'ARC',\n",
       " 'ARCB',\n",
       " 'ARCC',\n",
       " 'ARCE',\n",
       " 'ARCH',\n",
       " 'ARCO',\n",
       " 'ARCT',\n",
       " 'ARD',\n",
       " 'ARDC',\n",
       " 'ARDS',\n",
       " 'ARDX',\n",
       " 'ARE',\n",
       " 'AREC',\n",
       " 'ARES',\n",
       " 'ARES^A',\n",
       " 'ARGD',\n",
       " 'ARGO',\n",
       " 'ARGO^A',\n",
       " 'ARGX',\n",
       " 'ARI',\n",
       " 'ARKO',\n",
       " 'ARKOW',\n",
       " 'ARKR',\n",
       " 'ARKX',\n",
       " 'ARL',\n",
       " 'ARLO',\n",
       " 'ARLP',\n",
       " 'ARMK',\n",
       " 'ARMP',\n",
       " 'ARNA',\n",
       " 'ARNC',\n",
       " 'AROC',\n",
       " 'AROW',\n",
       " 'ARPO',\n",
       " 'ARQT',\n",
       " 'ARR',\n",
       " 'ARR^C',\n",
       " 'ARRW',\n",
       " 'ARRWU',\n",
       " 'ARRWW',\n",
       " 'ARRY',\n",
       " 'ARTL',\n",
       " 'ARTLW',\n",
       " 'ARTNA',\n",
       " 'ARTW',\n",
       " 'ARVL',\n",
       " 'ARVLW',\n",
       " 'ARVN',\n",
       " 'ARW',\n",
       " 'ARWR',\n",
       " 'ARYA',\n",
       " 'ARYD',\n",
       " 'ASA',\n",
       " 'ASAI',\n",
       " 'ASAN',\n",
       " 'ASAQ',\n",
       " 'ASAX',\n",
       " 'ASAXU',\n",
       " 'ASAXW',\n",
       " 'ASB',\n",
       " 'ASB^C',\n",
       " 'ASB^D',\n",
       " 'ASB^E',\n",
       " 'ASB^F',\n",
       " 'ASC',\n",
       " 'ASG',\n",
       " 'ASGI',\n",
       " 'ASGN',\n",
       " 'ASH',\n",
       " 'ASIX',\n",
       " 'ASLE',\n",
       " 'ASLEW',\n",
       " 'ASLN',\n",
       " 'ASM',\n",
       " 'ASMB',\n",
       " 'ASML',\n",
       " 'ASND',\n",
       " 'ASO',\n",
       " 'ASPC',\n",
       " 'ASPCU',\n",
       " 'ASPCW',\n",
       " 'ASPL',\n",
       " 'ASPN',\n",
       " 'ASPS',\n",
       " 'ASPU',\n",
       " 'ASR',\n",
       " 'ASRT',\n",
       " 'ASRV',\n",
       " 'ASRVP',\n",
       " 'ASTC',\n",
       " 'ASTE',\n",
       " 'ASTS',\n",
       " 'ASTSW',\n",
       " 'ASUR',\n",
       " 'ASX',\n",
       " 'ASXC',\n",
       " 'ASYS',\n",
       " 'ASZ',\n",
       " 'AT',\n",
       " 'ATA',\n",
       " 'ATAC',\n",
       " 'ATAQ',\n",
       " 'ATAX',\n",
       " 'ATC',\n",
       " 'ATCO',\n",
       " 'ATCO^D',\n",
       " 'ATCO^E',\n",
       " 'ATCO^G',\n",
       " 'ATCO^H',\n",
       " 'ATCO^I',\n",
       " 'ATCX',\n",
       " 'ATEC',\n",
       " 'ATEN',\n",
       " 'ATER',\n",
       " 'ATEX',\n",
       " 'ATGE',\n",
       " 'ATH',\n",
       " 'ATH^A',\n",
       " 'ATH^B',\n",
       " 'ATH^C',\n",
       " 'ATH^D',\n",
       " 'ATHA',\n",
       " 'ATHE',\n",
       " 'ATHM',\n",
       " 'ATHN',\n",
       " 'ATHX',\n",
       " 'ATI',\n",
       " 'ATIF',\n",
       " 'ATKR',\n",
       " 'ATLC',\n",
       " 'ATLO',\n",
       " 'ATMR',\n",
       " 'ATNF',\n",
       " 'ATNFW',\n",
       " 'ATNI',\n",
       " 'ATNM',\n",
       " 'ATNX',\n",
       " 'ATO',\n",
       " 'ATOM',\n",
       " 'ATOS',\n",
       " 'ATR',\n",
       " 'ATRA',\n",
       " 'ATRC',\n",
       " 'ATRI',\n",
       " 'ATRO',\n",
       " 'ATRS',\n",
       " 'ATSG',\n",
       " 'ATSPT',\n",
       " 'ATSPU',\n",
       " 'ATTO',\n",
       " 'ATUS',\n",
       " 'ATVCU',\n",
       " 'ATVI',\n",
       " 'ATXI',\n",
       " 'AU',\n",
       " 'AUB',\n",
       " 'AUBAP',\n",
       " 'AUBN',\n",
       " 'AUD',\n",
       " 'AUDC',\n",
       " 'AUMN',\n",
       " 'AUPH',\n",
       " 'AURC',\n",
       " 'AURCU',\n",
       " 'AUS',\n",
       " 'AUTL',\n",
       " 'AUTO',\n",
       " 'AUUD',\n",
       " 'AUUDW',\n",
       " 'AUVI',\n",
       " 'AUY',\n",
       " 'AVA',\n",
       " 'AVAH',\n",
       " 'AVAL',\n",
       " 'AVAN',\n",
       " 'AVAV',\n",
       " 'AVB',\n",
       " 'AVCO',\n",
       " 'AVCT',\n",
       " 'AVCTW',\n",
       " 'AVD',\n",
       " 'AVDG',\n",
       " 'AVDL',\n",
       " 'AVDR',\n",
       " 'AVEO',\n",
       " 'AVGO',\n",
       " 'AVGOP',\n",
       " 'AVGR',\n",
       " 'AVID',\n",
       " 'AVIR',\n",
       " 'AVK',\n",
       " 'AVLR',\n",
       " 'AVNS',\n",
       " 'AVNT',\n",
       " 'AVNW',\n",
       " 'AVO',\n",
       " 'AVRO',\n",
       " 'AVT',\n",
       " 'AVTR',\n",
       " 'AVTR^A',\n",
       " 'AVXL',\n",
       " 'AVY',\n",
       " 'AVYA',\n",
       " 'AWF',\n",
       " 'AWH',\n",
       " 'AWI',\n",
       " 'AWK',\n",
       " 'AWP',\n",
       " 'AWR',\n",
       " 'AWRE',\n",
       " 'AWX',\n",
       " 'AX',\n",
       " 'AXAS',\n",
       " 'AXDX',\n",
       " 'AXGN',\n",
       " 'AXL',\n",
       " 'AXLA',\n",
       " 'AXNX',\n",
       " 'AXON',\n",
       " 'AXP',\n",
       " 'AXR',\n",
       " 'AXS',\n",
       " 'AXS^E',\n",
       " 'AXSM',\n",
       " 'AXTA',\n",
       " 'AXTI',\n",
       " 'AXU',\n",
       " 'AY',\n",
       " 'AYI',\n",
       " 'AYLA',\n",
       " 'AYRO',\n",
       " 'AYTU',\n",
       " 'AYX',\n",
       " 'AZEK',\n",
       " 'AZN',\n",
       " 'AZO',\n",
       " 'AZPN',\n",
       " 'AZRE',\n",
       " 'AZRX',\n",
       " 'AZUL',\n",
       " 'AZYO',\n",
       " 'AZZ',\n",
       " 'B',\n",
       " 'BA',\n",
       " 'BABA',\n",
       " 'BAC',\n",
       " 'BAC^B',\n",
       " 'BAC^E',\n",
       " 'BAC^K',\n",
       " 'BAC^L',\n",
       " 'BAC^M',\n",
       " 'BAC^N',\n",
       " 'BAC^O',\n",
       " 'BAC^P',\n",
       " 'BAH',\n",
       " 'BAK',\n",
       " 'BALY',\n",
       " 'BAM',\n",
       " 'BAMH',\n",
       " 'BAMI',\n",
       " 'BANC',\n",
       " 'BANC^E',\n",
       " 'BAND',\n",
       " 'BANF',\n",
       " 'BANR',\n",
       " 'BANX',\n",
       " 'BAOS',\n",
       " 'BAP',\n",
       " 'BATL',\n",
       " 'BATRA',\n",
       " 'BATRK',\n",
       " 'BAX',\n",
       " 'BB',\n",
       " 'BBAR',\n",
       " 'BBBY',\n",
       " 'BBCP',\n",
       " 'BBD',\n",
       " 'BBDC',\n",
       " 'BBDO',\n",
       " 'BBGI',\n",
       " 'BBI',\n",
       " 'BBIG',\n",
       " 'BBIO',\n",
       " 'BBL',\n",
       " 'BBN',\n",
       " 'BBQ',\n",
       " 'BBSI',\n",
       " 'BBU',\n",
       " 'BBVA',\n",
       " 'BBW',\n",
       " 'BBY',\n",
       " 'BC',\n",
       " 'BC^A',\n",
       " 'BC^B',\n",
       " 'BC^C',\n",
       " 'BCAB',\n",
       " 'BCAC',\n",
       " 'BCACU',\n",
       " 'BCACW',\n",
       " 'BCAT',\n",
       " 'BCBP',\n",
       " 'BCC',\n",
       " 'BCDA',\n",
       " 'BCDAW',\n",
       " 'BCE',\n",
       " 'BCEI',\n",
       " 'BCEL',\n",
       " 'BCH',\n",
       " 'BCLI',\n",
       " 'BCML',\n",
       " 'BCO',\n",
       " 'BCOR',\n",
       " 'BCOV',\n",
       " 'BCOW',\n",
       " 'BCPC',\n",
       " 'BCRX',\n",
       " 'BCS',\n",
       " 'BCSF',\n",
       " 'BCTG',\n",
       " 'BCTX',\n",
       " 'BCTXW',\n",
       " 'BCV',\n",
       " 'BCV^A',\n",
       " 'BCX',\n",
       " 'BCYC',\n",
       " 'BCYP',\n",
       " 'BCYPU',\n",
       " 'BCYPW',\n",
       " 'BDC',\n",
       " 'BDJ',\n",
       " 'BDL',\n",
       " 'BDN',\n",
       " 'BDR',\n",
       " 'BDSI',\n",
       " 'BDSX',\n",
       " 'BDTX',\n",
       " 'BDX',\n",
       " 'BDXB',\n",
       " 'BE',\n",
       " 'BEAM',\n",
       " 'BECN',\n",
       " 'BEDU',\n",
       " 'BEEM',\n",
       " 'BEEMW',\n",
       " 'BEKE',\n",
       " 'BELFA',\n",
       " 'BELFB',\n",
       " 'BEN',\n",
       " 'BENE',\n",
       " 'BENER',\n",
       " 'BENEU',\n",
       " 'BENEW',\n",
       " 'BEP',\n",
       " 'BEP^A',\n",
       " 'BEPC',\n",
       " 'BEPH',\n",
       " 'BERY',\n",
       " 'BEST',\n",
       " 'BF/A',\n",
       " 'BF/B',\n",
       " 'BFAM',\n",
       " 'BFC',\n",
       " 'BFI',\n",
       " 'BFIIW',\n",
       " 'BFIN',\n",
       " 'BFK',\n",
       " 'BFLY',\n",
       " 'BFRA',\n",
       " 'BFS',\n",
       " 'BFS^D',\n",
       " 'BFS^E',\n",
       " 'BFST',\n",
       " 'BFZ',\n",
       " 'BG',\n",
       " 'BGB',\n",
       " 'BGCP',\n",
       " 'BGFV',\n",
       " 'BGH',\n",
       " 'BGI',\n",
       " 'BGIO',\n",
       " 'BGLD',\n",
       " 'BGNE',\n",
       " 'BGR',\n",
       " 'BGS',\n",
       " 'BGSF',\n",
       " 'BGSX',\n",
       " 'BGT',\n",
       " 'BGX',\n",
       " 'BGY',\n",
       " 'BH',\n",
       " 'BHAT',\n",
       " 'BHB',\n",
       " 'BHC',\n",
       " 'BHE',\n",
       " 'BHF',\n",
       " 'BHFAL',\n",
       " 'BHFAN',\n",
       " 'BHFAO',\n",
       " 'BHFAP',\n",
       " 'BHK',\n",
       " 'BHLB',\n",
       " 'BHP',\n",
       " 'BHR',\n",
       " 'BHR^B',\n",
       " 'BHR^D',\n",
       " 'BHSE',\n",
       " 'BHSEW',\n",
       " 'BHTG',\n",
       " 'BHV',\n",
       " 'BHVN',\n",
       " 'BIDU',\n",
       " 'BIF',\n",
       " 'BIG',\n",
       " 'BIGC',\n",
       " 'BIGZ',\n",
       " 'BIIB',\n",
       " 'BILI',\n",
       " 'BILL',\n",
       " 'BIMI',\n",
       " 'BIO',\n",
       " 'BIO/B',\n",
       " 'BIOC',\n",
       " 'BIOL',\n",
       " 'BIOT',\n",
       " 'BIOTU',\n",
       " 'BIOTW',\n",
       " 'BIOX',\n",
       " 'BIP',\n",
       " 'BIP^A',\n",
       " 'BIP^B',\n",
       " 'BIPC',\n",
       " 'BIT',\n",
       " 'BITE',\n",
       " 'BIVI',\n",
       " 'BJ',\n",
       " 'BJRI',\n",
       " 'BK',\n",
       " 'BKCC',\n",
       " 'BKD',\n",
       " 'BKE',\n",
       " 'BKEP',\n",
       " 'BKEPP',\n",
       " 'BKH',\n",
       " 'BKI',\n",
       " 'BKN',\n",
       " 'BKNG',\n",
       " 'BKR',\n",
       " 'BKSC',\n",
       " 'BKT',\n",
       " 'BKTI',\n",
       " 'BKU',\n",
       " 'BKYI',\n",
       " 'BL',\n",
       " 'BLBD',\n",
       " 'BLCM',\n",
       " 'BLCT',\n",
       " 'BLD',\n",
       " 'BLDG',\n",
       " 'BLDP',\n",
       " 'BLDR',\n",
       " 'BLE',\n",
       " 'BLFS',\n",
       " 'BLI',\n",
       " 'BLIN',\n",
       " 'BLK',\n",
       " 'BLKB',\n",
       " 'BLL',\n",
       " 'BLMN',\n",
       " 'BLNK',\n",
       " 'BLNKW',\n",
       " 'BLPH',\n",
       " 'BLRX',\n",
       " 'BLSA',\n",
       " 'BLTS',\n",
       " 'BLTSU',\n",
       " 'BLTSW',\n",
       " 'BLU',\n",
       " ...]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of tickers that are in the above dataframe\n",
    "tickers = ticker_df['Term'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "hungarian-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Vader to analyse social media sentiment better than with nltk\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "numerous-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    try:\n",
    "        scores = sia.polarity_scores(text)\n",
    "        return  scores['compound']\n",
    "    except: return none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "religious-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polarity column to posts\n",
    "df_posts['polarity'] = df_posts['text'].apply(sentiment)\n",
    "# drop title column\n",
    "df_posts = df_posts.drop(['title'], axis=1)\n",
    "# add polarity column \n",
    "df_comments['polarity'] = df_comments['comments'].apply(sentiment)\n",
    "df_comments_move['polarity'] = df_comments_move['comments'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "associate-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of the three dataframes to make iterating for sentiment extraction easier\n",
    "list_df = [df_posts, df_comments, df_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "controlling-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting sentiment and associated ticker out of the dataframe\n",
    "sentiment_to_df= [] # list for sentiment score\n",
    "tickers_to_df = [] # list for tickers\n",
    "\n",
    "for dataframe in list_df: \n",
    "    for index, row in dataframe.iterrows(): # for every row in dataframe\n",
    "        title_words = row[0].split(' ') # split comment at whitespace and put into list\n",
    "        for word in title_words:\n",
    "            if word in tickers: \n",
    "                tickers_to_df.append(word) # append the word to the word list\n",
    "                sentiment_to_df.append(row[1]) # append the sentiment to the sentiment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "alien-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentiment dataframe \n",
    "sentiment_df1 = pd.DataFrame(list(zip(tickers_to_df, sentiment_to_df)),\n",
    "              columns =['Term', 'Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "mounted-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group table by ticker, get mean of sentiment, create new dataframe with new index\n",
    "sentiment_table=sentiment_df1.groupby(['Term']).mean()\n",
    "sentiment_table['Term'] = sentiment_table.index\n",
    "sentiment_table=sentiment_table.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "colored-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "sentiment_table.to_csv('sentiment_of_tickers_' + str(now.strftime(\"%d\"))+'_'+str(now.strftime(\"%m\"))+ '_'+ str(now.strftime(\"%H\"))+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
